{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RNN$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t = Activation(W_hh_{t-1}+W_xx_t+b)$\n",
    "\n",
    "$y_t = W_yh_t + c$\n",
    "\n",
    "**Significado :**\n",
    "\n",
    "* $h_t$ : estado oculto en el momento $t$, es una especie de memoria de RNN\n",
    "* $x_t$ : es la entrada en el momento $t$\n",
    "* $W_h$ es el peso para el estado oculto anterior (memoria) y $W_x$ es el peso para la entrada actual\n",
    "* $W_y$ es el peso para la salida \n",
    "* $b$ es el sesgo y $c$ es un posible ajuste de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagina que tienes una secuencia de entradas de palabras, y queremos que nuestra RNN prediga si cada palabra es positiva o negativa. Usaremos un caso muy simplificado, con solo tres palabras: \"me\", \"gusta\", \"mucho\". Asumimos que cada palabra es representada por un número (su codificación), y el objetivo es que la red aprenda a clasificar si la secuencia es positiva o negativa. Aquí es cómo lo haríamos:\n",
    "\n",
    "* 'me' ->  $x_1$  = 0.1\n",
    "* 'gusta' ->  $x_2$  = 0.5\n",
    "* 'mucho' ->  $x_3$  = 0.7\n",
    "\n",
    "Cada palabra tiene una representacion numerica\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   Paso 1\n",
    "\n",
    "Estado oculto $h_0=0$ ya que al principio no tenemos ninguna memoria.\n",
    "\n",
    ">   Paso 2 ($t=1$)\n",
    "\n",
    "$h_t = Activation(W_hh_{t-1}+W_xx_t+b)$\n",
    "\n",
    "Primer paso, la RNN recibe de entrada una $X1 = 0.1$ t actualiza su estado oculto $h_1$\n",
    "\n",
    "se simplica y se usa una $f(x)$ de activacion lineal como ejemplo de simplicidad, y unos valores arbitrarios de pesos:\n",
    "\n",
    "* $W_h =  0.8$ (peso para el estado oculto anterior) \n",
    "* $W_x =  0.5$ (peso para la entrada actual) \n",
    "* $b = 0.1$ (sesgo)\n",
    "\n",
    "$h_1 = Activation(0.8⋅h_{0}+0.5⋅x_1+0.1)$\n",
    "\n",
    "sustituyendo:\n",
    "\n",
    "$h_1 = Activation(0.8⋅0+0.5⋅0.1+0.1) = 0.15$\n",
    "\n",
    "Entonces el estado oculto despues del primer paso es $h_1 = 0.15$\n",
    "\n",
    ">   Paso 3 ($t_2$)\n",
    "\n",
    "Ahora, en el segundo paso, la RNN recibe la entrada $x_2=0.5$ (la palabra 'gusta'), y actualiza su estado oculto \n",
    "\n",
    "$h_2 = Activation(0.8⋅h_{1}+0.5⋅x_2+0.1)$\n",
    "\n",
    "sustituyendo:\n",
    "\n",
    "$h_2 = Activation(0.8⋅0.15+0.5⋅0.5+0.1) = 0.47$\n",
    "\n",
    ">   Paso 4 ($t_3$)\n",
    "\n",
    "Ahora en el tercer paso, l aRNN recibe $x_3 = 0.7$ (la palabra  'mucho' ) y actualiza el estado oculto $h_3$ utilizando el estado anterior $h_2 = 0.47$:\n",
    "\n",
    "$h_3 = Activation(0.8⋅h_{2}+0.5⋅x_3+0.1)$\n",
    "\n",
    "sustituyendo:\n",
    "\n",
    "$h_3 = Activation(0.8⋅0.47+0.5⋅0.7+0.1) = 0.826$\n",
    "\n",
    ">       Paso 5 - Prediccon\n",
    "\n",
    "Finalmente podemos hacer una prediccion de la salida usando el estado oculto final $h_3 = 0.826$. Supongamos que la salida se calcula de la sig manera:\n",
    "\n",
    "$y_t=W_y⋅h_t + c$\n",
    "\n",
    "Donde usamos un peso $W_y=1.2$ y yn sesgo $c=0.2$:\n",
    "\n",
    "$y_3 = 1.2⋅h_3+0.2=1.2⋅0.826+0.2=0.9912+0.2=1.1912$\n",
    "\n",
    "Si tomamos una salida mayor que $0.5$ como positiva, la salida es positiva.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Through Time (BPTT)\n",
    "\n",
    "en lugar de hacer la retropropagación solo para la salida final, se hace a través de todos los pasos de tiempo. Esto significa que el gradiente se calcula y se actualizan los pesos de la red considerando todas las secuencias pasadas (a través de los $h_t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">       Ejemplo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definicion y notaciones*\n",
    "\n",
    "Se usara la secuencia simple:\n",
    "\n",
    "$x_1 = 1,x_2 = 2,x_3 = 3$\n",
    "\n",
    "lo cual queremos predecir que la RNN prediga 4 despues de la secuencia $x_1,x_2,x_3$\n",
    "\n",
    "1. Pesos iniciales ($arbitriarios$):\n",
    "\n",
    "* $W_h=0.5$ \n",
    "* $W_x=0.8$ \n",
    "* $W_y=1.0$ \n",
    "* $Sesgo:b=0.2$ \n",
    "* Tasa de aprendizaje : $\\eta=0.1$ \n",
    "\n",
    "2. Estado oculto inicial $h_0=0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Paso 1 : *Propagacion hacia delante (Forwrd Pass)*\n",
    "\n",
    "La propagacion hacia delante seran 3 pasos:  $x_1,x_2,x_3$. Para cada uno de estos pasos, calculeremos el estado oculto y la salida.\n",
    "\n",
    "> > *iteracion 1* : $t=1$(Para $x_1=1$)\n",
    "1. Estado oculto $h_1$:\n",
    "\n",
    "$h_1: Activacion(W_h⋅h_0+W_x⋅x_1+b)$\n",
    "\n",
    "Sustituimos valores:\n",
    "\n",
    "$h_1: Activacion(0.5⋅0.0+0.8⋅1.0+0.2)$ = *Activacion*($1.0$) \n",
    "\n",
    "Si usamos una activacion lineal (para simplicar), entonces:\n",
    "\n",
    "$h_1 = 1.0$\n",
    "\n",
    "\n",
    "2. Salida $y_1$\n",
    "\n",
    "$y_1 = W_y⋅h_1=1.0⋅1.0=1.0$\n",
    "\n",
    "> > *iteracion 2* : $t=2$(Para $x_2=2$)\n",
    "1. Estado oculto $h_2$:\n",
    "\n",
    "$h_2: Activacion(W_h⋅h_1+W_x⋅x_2+b)$\n",
    "\n",
    "Sustituimos valores:\n",
    "\n",
    "$h_1: Activacion(0.5⋅0.1+0.8⋅2.0+0.2)$ = *Activacion*($2.3$) \n",
    "\n",
    "Si usamos una activacion lineal (para simplicar), entonces:\n",
    "\n",
    "$h_2 = 2.3$\n",
    "\n",
    "\n",
    "2. Salida $y_2$\n",
    "\n",
    "$y_2 = W_y⋅h_2=1.0⋅2.3=2.3$\n",
    "\n",
    "> > *iteracion 3* : $t=3$(Para $x_3=3$)\n",
    "1. Estado oculto $h_3$:\n",
    "\n",
    "$h_3: Activacion(W_h⋅h_2+W_x⋅x_3+b)$\n",
    "\n",
    "Sustituimos valores:\n",
    "\n",
    "$h_3: Activacion(0.5⋅2.3+0.8⋅3.0+0.2)$ = *Activacion*($3.75$) \n",
    "\n",
    "Si usamos una activacion lineal (para simplicar), entonces:\n",
    "\n",
    "$h_3 = 3.75$\n",
    "\n",
    "\n",
    "2. Salida $y_2$\n",
    "\n",
    "$y_3 = W_y⋅h_2=1.0⋅3.75=3.75$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > Paso 2 : *Cálculo del Error* $(Loss)$\n",
    "\n",
    "$Error: \\frac{1}{2}⋅(y_{deseado}-y_3)^2$\n",
    "\n",
    "Sustituyendo los valores\n",
    "\n",
    "$Error: \\frac{1}{2}⋅(4-3.75)^2=0.03125$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > Paso 3 :  *Backpropagation Through Time (BPTT)*\n",
    "\n",
    "Usaremos *BPTT* para ajustar los pesos. Empezaremos por calcular el gradiante del error respecto a la salida $y_3$\n",
    "\n",
    "Gradiante de *Error* respecto a $y_3$ : \n",
    "\n",
    "$\\frac{\\partial Error}{\\partial y_3} = (y_3-y_{deseado}) = 3.75 -4 = -0.25$\n",
    "\n",
    "Gradiante de *Error* respecto a $W_y$ :\n",
    "\n",
    "$\\frac{\\partial y_3}{\\partial W_y} = h_3$\n",
    "\n",
    "Por lo que:\n",
    "\n",
    "$\\frac{\\partial Error}{\\partial W_y}=\\frac{\\partial Error}{\\partial y_3}⋅\\frac{\\partial y_3}{\\partial W_y} = -0.25 ⋅ 3.75 = -0.9375$ \n",
    "\n",
    "Actualizacion de $W_y$ : \n",
    "\n",
    "Utilizando la tasa de aprendizaje $\\eta = 0.1$ actualizaremos el peso de $W_y$:\n",
    "\n",
    "$W_y \\leftarrow W_y - \\eta ⋅ \\frac{\\partial Error}{\\partial W_y}$\n",
    "\n",
    "Sustituimos\n",
    "\n",
    "$W_y \\leftarrow 1.0 - 0.1 ⋅ (-0.9375) = 1.0 + 0.09375 = 1.09375$\n",
    "\n",
    "Gradiante de *Error* respecto a $h_3$\n",
    "\n",
    "Ahora, propagamos el error hacia atras a $h_3$\n",
    "\n",
    "$\\frac{\\partial Error}{\\partial h_3} = \\frac{\\partial Error}{\\partial y_3}⋅\\frac{\\partial y_3}{\\partial h_3} = -0.25 ⋅ 1.0 = -0.25$\n",
    "\n",
    "Gradiante Error respecto a $W_h$ y $W_x$\n",
    "\n",
    "Para continuar con el *BPTT*, necesitamos calcular como el error se propaga hacia atras atraves de $h_2$ y $x_3$\n",
    "\n",
    "1. Gradiante de $h_3$ respecto a $h_2$ \n",
    "\n",
    "$\\frac{\\partial h_3}{\\partial h_2} = W_h$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*first test to uso merge in main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.739329289302358\n",
      "Epoch 2, Loss: 5.13651660232748\n",
      "Epoch 3, Loss: 4.907406089141977\n",
      "Epoch 4, Loss: 4.750404289693494\n",
      "Epoch 5, Loss: 4.603041240434077\n",
      "Epoch 6, Loss: 4.458242679530893\n",
      "Epoch 7, Loss: 4.315698359326397\n",
      "Epoch 8, Loss: 4.175477303627954\n",
      "Epoch 9, Loss: 4.037600841513564\n",
      "Epoch 10, Loss: 3.9020829750544954\n",
      "Epoch 11, Loss: 3.7689526254652765\n",
      "Epoch 12, Loss: 3.638256881145232\n",
      "Epoch 13, Loss: 3.5100587127471794\n",
      "Epoch 14, Loss: 3.384433765648229\n",
      "Epoch 15, Loss: 3.2614673383448944\n",
      "Epoch 16, Loss: 3.1412517156972464\n",
      "Epoch 17, Loss: 3.023883828577013\n",
      "Epoch 18, Loss: 2.909463188743182\n",
      "Epoch 19, Loss: 2.798090055853711\n",
      "Epoch 20, Loss: 2.689863802300356\n",
      "Epoch 21, Loss: 2.58488144714694\n",
      "Epoch 22, Loss: 2.483236333731033\n",
      "Epoch 23, Loss: 2.3850169275825683\n",
      "Epoch 24, Loss: 2.290305712982898\n",
      "Epoch 25, Loss: 2.199178168209307\n",
      "Epoch 26, Loss: 2.1117018015877216\n",
      "Epoch 27, Loss: 2.0279352331115934\n",
      "Epoch 28, Loss: 1.9479273097022738\n",
      "Epoch 29, Loss: 1.8717162462404084\n",
      "Epoch 30, Loss: 1.7993287892710836\n",
      "Epoch 31, Loss: 1.7307794056817924\n",
      "Epoch 32, Loss: 1.6660695044936529\n",
      "Epoch 33, Loss: 1.6051867059311433\n",
      "Epoch 34, Loss: 1.5481041778050835\n",
      "Epoch 35, Loss: 1.4947800645536302\n",
      "Epoch 36, Loss: 1.4451570385903534\n",
      "Epoch 37, Loss: 1.3991620064500665\n",
      "Epoch 38, Loss: 1.3567060031742475\n",
      "Epoch 39, Loss: 1.3176843070876882\n",
      "Epoch 40, Loss: 1.2819768033594134\n",
      "Epoch 41, Loss: 1.2494486184561062\n",
      "Epoch 42, Loss: 1.2199510389303985\n",
      "Epoch 43, Loss: 1.1933227173030432\n",
      "Epoch 44, Loss: 1.1693911556745895\n",
      "Epoch 45, Loss: 1.1479744448960651\n",
      "Epoch 46, Loss: 1.1288832245210152\n",
      "Epoch 47, Loss: 1.1119228172803903\n",
      "Epoch 48, Loss: 1.0968954823507262\n",
      "Epoch 49, Loss: 1.0836027249746256\n",
      "Epoch 50, Loss: 1.071847596575986\n",
      "Epoch 51, Loss: 1.0614369196567093\n",
      "Epoch 52, Loss: 1.0521833754353778\n",
      "Epoch 53, Loss: 1.0439073990721366\n",
      "Epoch 54, Loss: 1.0364388368496924\n",
      "Epoch 55, Loss: 1.0296183310977367\n",
      "Epoch 56, Loss: 1.0232984111069086\n",
      "Epoch 57, Loss: 1.0173442809151516\n",
      "Epoch 58, Loss: 1.0116343068659792\n",
      "Epoch 59, Loss: 1.0060602185679284\n",
      "Epoch 60, Loss: 1.0005270458322086\n",
      "Epoch 61, Loss: 0.9949528210286631\n",
      "Epoch 62, Loss: 0.9892680809644793\n",
      "Epoch 63, Loss: 0.9834152049100067\n",
      "Epoch 64, Loss: 0.9773476259608734\n",
      "Epoch 65, Loss: 0.9710289518206443\n",
      "Epoch 66, Loss: 0.9644320286544368\n",
      "Epoch 67, Loss: 0.9575379782610552\n",
      "Epoch 68, Loss: 0.9503352347878493\n",
      "Epoch 69, Loss: 0.9428186028831809\n",
      "Epoch 70, Loss: 0.9349883548126935\n",
      "Epoch 71, Loss: 0.9268493798683457\n",
      "Epoch 72, Loss: 0.91841039552843\n",
      "Epoch 73, Loss: 0.9096832263845241\n",
      "Epoch 74, Loss: 0.9006821538933962\n",
      "Epoch 75, Loss: 0.8914233375563582\n",
      "Epoch 76, Loss: 0.8819243061637239\n",
      "Epoch 77, Loss: 0.8722035162353974\n",
      "Epoch 78, Loss: 0.8622799736937377\n",
      "Epoch 79, Loss: 0.8521729140678675\n",
      "Epoch 80, Loss: 0.8419015360928916\n",
      "Epoch 81, Loss: 0.8314847833778398\n",
      "Epoch 82, Loss: 0.8209411688203754\n",
      "Epoch 83, Loss: 0.8102886365981303\n",
      "Epoch 84, Loss: 0.7995444568250264\n",
      "Epoch 85, Loss: 0.7887251482920712\n",
      "Epoch 86, Loss: 0.777846425087447\n",
      "Epoch 87, Loss: 0.7669231632879399\n",
      "Epoch 88, Loss: 0.7559693843153495\n",
      "Epoch 89, Loss: 0.7449982519444673\n",
      "Epoch 90, Loss: 0.7340220803243142\n",
      "Epoch 91, Loss: 0.723052350725191\n",
      "Epoch 92, Loss: 0.7120997350470774\n",
      "Epoch 93, Loss: 0.7011741244179784\n",
      "Epoch 94, Loss: 0.6902846614736393\n",
      "Epoch 95, Loss: 0.6794397751431307\n",
      "Epoch 96, Loss: 0.6686472169695984\n",
      "Epoch 97, Loss: 0.6579140981737\n",
      "Epoch 98, Loss: 0.6472469268211537\n",
      "Epoch 99, Loss: 0.6366516445875745\n",
      "Epoch 100, Loss: 0.6261336627257522\n",
      "Predicción final: 4.679519606047104\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inicialización de parámetros\n",
    "W_h = 0.5  # Peso entre el estado oculto anterior y el actual\n",
    "W_x = 0.8  # Peso entre la entrada y el estado oculto\n",
    "W_y = 1.0  # Peso entre el estado oculto y la salida\n",
    "b = 0.2    # Sesgo\n",
    "eta = 0.01  # Tasa de aprendizaje\n",
    "h_0 = 0     # Estado oculto inicial\n",
    "\n",
    "# Función de activación (usamos una activación lineal simple)\n",
    "def activation(x):\n",
    "    return x\n",
    "\n",
    "# Función de pérdida (error cuadrático medio)\n",
    "def loss(y_pred, y_true):\n",
    "    return 0.5 * (y_pred - y_true)**2\n",
    "\n",
    "# Propagación hacia adelante\n",
    "def forward_pass(x, h_prev):\n",
    "    h = activation(W_h * h_prev + W_x * x + b)  # Cálculo del estado oculto\n",
    "    y = W_y * h  # Cálculo de la salida\n",
    "    return h, y\n",
    "\n",
    "# Cálculo del gradiente de la pérdida con respecto a los pesos\n",
    "def backward_pass(x, h_prev, h, y_pred, y_true):\n",
    "    # Gradiente del error respecto a la salida\n",
    "    dL_dy = y_pred - y_true\n",
    "    \n",
    "    # Gradientes respecto a los pesos\n",
    "    dL_dW_y = dL_dy * h\n",
    "    dL_dh = dL_dy * W_y  # Propagación hacia atrás a través de h\n",
    "    \n",
    "    # Gradientes respecto a los otros pesos\n",
    "    dL_dW_h = dL_dh * h_prev\n",
    "    dL_dW_x = dL_dh * x\n",
    "    dL_db = dL_dh\n",
    "    \n",
    "    return dL_dW_y, dL_dW_h, dL_dW_x, dL_db, dL_dh\n",
    "\n",
    "# Actualización de los pesos\n",
    "def update_weights(W_y, W_h, W_x, b, dL_dW_y, dL_dW_h, dL_dW_x, dL_db):\n",
    "    W_y -= eta * dL_dW_y\n",
    "    W_h -= eta * dL_dW_h\n",
    "    W_x -= eta * dL_dW_x\n",
    "    b -= eta * dL_db\n",
    "    return W_y, W_h, W_x, b\n",
    "\n",
    "# Datos de entrada (secuencia de 3 pasos)\n",
    "X = [1, 2, 3]  # Entrada\n",
    "y_true = 4  # Valor deseado para la predicción final\n",
    "\n",
    "# Entrenamiento - Iteraciones\n",
    "for epoch in range(100):  # Número de épocas\n",
    "    h_prev = h_0\n",
    "    total_loss = 0\n",
    "\n",
    "    # Propagación hacia adelante y Backpropagation para cada paso de tiempo\n",
    "    for t in range(len(X)):\n",
    "        x_t = X[t]\n",
    "        \n",
    "        # Paso hacia adelante\n",
    "        h, y_pred = forward_pass(x_t, h_prev)\n",
    "        \n",
    "        # Calcular la pérdida\n",
    "        total_loss += loss(y_pred, y_true)\n",
    "        \n",
    "        # Backpropagation\n",
    "        dL_dW_y, dL_dW_h, dL_dW_x, dL_db, dL_dh = backward_pass(x_t, h_prev, h, y_pred, y_true)\n",
    "        \n",
    "        # Actualizar los pesos\n",
    "        W_y, W_h, W_x, b = update_weights(W_y, W_h, W_x, b, dL_dW_y, dL_dW_h, dL_dW_x, dL_db)\n",
    "        \n",
    "        # Actualizar el estado oculto\n",
    "        h_prev = h\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")\n",
    "\n",
    "# Predicción final al terminar el entrenamiento\n",
    "y_pred_final = W_y * h_prev  # h_prev es el último estado oculto calculado\n",
    "print(f\"Predicción final: {y_pred_final}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
